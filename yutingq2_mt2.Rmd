---
title: "yutingq2_midterm"
author: "Yuting Qiu"
date: "11/14/2019"
output: pdf_document
---

Overall grade : 101 ( main = 96 + bonus = 5 )
Part 1: -2 points from question 7
Part 2: -2 points from coding errors

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(lme4)
library(lmerTest)
library(lsmeans)
library(reshape2)
library(dplyr)
library(ggplot2)
library(glmnet)
library(caret)
library(MASS)
library(tidyverse)


read.table("examFile.csv", sep=",",header= TRUE,na.strings = "NA")->df
```



## PART 1 ## Environmental Effects Evaluation
## Submit a visual and output for each question below
# 1.Is location effect significant? 
# a. What fraction of the variation observed in yield is attributable to Location specific effects?
# b. Which location seems to be the highest yield location?
```{r}
model1 <- lm(Estimate~Loc, data=df)
anova(model1)
## yes, location effect is significant

summary(model1)
# 79.25% of the variation observed in the yield is attributable to location specific effect

aggregate(x=df$Estimate,
          by=list(df$Loc),
          FUN=mean)
## Hampshire_2019 is the highest yield location
```


# 2. Is Company effect significant?  Justify your answer.
# a. Which company's varieties seem to perform the best across all regions?
# b. Which company's varieties seem to perform the worst across all regions?
```{r}
model2 <- lm(Estimate~Company, df)
anova(model2)
# yes, company effect significant

aggregate(x=df$Estimate,
          by=list(df$Company),
          FUN=mean) %>% 
  as.data.frame() -> ag2
# Moiner_Seed variaty perform the best across all regions
# Go_Wheat variaty perform the worst across all regions
```
## EE comment - for the above question, you can actually code your output.
ag2$Group.1[which(ag2$x == max(ag2$x))]
ag2$Group.1[which(ag2$x == min(ag2$x))]

# 3. Is Region effect significant? How much variation in yield does region explain alone?
# How about together with Company? Justify your answer
# c.Which company's varieties seem to perform the best within each region?
# d.Which company's varieties seem to perform the worst within  each region?
```{r}
model3.1 <- lm(Estimate ~ Reg, df)
anova(model3.1)
summary(model3.1)
# Region effect is significant, 16.24% of the variation in yield explained by region alone.

model3.2 <- lm(Estimate ~ Reg + Company, df)
anova(model3.2)
summary(model3.2)
# According to the ANOVA result, region effect is significant with company.

library(dplyr)
as.data.frame(aggregate(x=df$Estimate,
          by=list(df$Reg,df$Company),
          FUN=mean)) -> ag3

# perform the best
# west	DeRaedt_Seed
# east	Moiner_Seed

# perfrom the worst
# west	ProHarvest
# east	Miller_Bros_Farm_and_Fert.
```


# 4. Is location effect significant together with Company and Region?
# e.Which company's varieties seem to perform the best for each location?
# f.Which company's varieties seem to perform the worst for each location?
```{r}
model4.1<- lm(Estimate ~ Reg + Company + Loc, df)
anova(model4.1)
# location effect is significant together with company and region

as.data.frame(aggregate(x=df$Estimate,
          by=list(df$Loc,df$Company),
          FUN=mean)) -> ag4

# perform the best
# Hampshire_2019	Pioneer
# Elkville_2019	KWS_Cereals
# Urbana_2018	Pioneer
# Perry_2019	Pioneer
# Neoga_2018	Croplan
# Belleville_2019	Pioneer

# perfrom the worst
# Belleville_2019	ProHarvest
# Neoga_2018	Miller_Bros_Farm_and_Fert.
# Elkville_2019	ProHarvest
# Perry_2019	ProHarvest
# Urbana_2018	Kratz_Farms
# Hampshire_2019	Kratz_Farms
```
EE comment : So How do I get your answer as the output?
first make a list of the location names
loc_list<- unique(ag4$Group.1)
then loop through the list- printing max value per location

for( i in ( 1:length(loc_list))){
c.loc<-loc_list[i]
c.val<-ag4[which(ag4$Group.1 == c.loc),]
print(c.val[which(c.val$x == max(c.val$x)),])
print(c.val[which(c.val$x == min(c.val$x)),])
}

# 5. Does the seed treatments have a significant effect on the yield?
# a.Which treatment seems to have the largest positive effect? Is it significant?
# b.What fraction of the variation observed in yield is attributable to seed treatments?
```{r}
model5.1 <- lm(Estimate ~ SeedTreatment, df)
anova(model5.1)
summary(model5.1)
# yes, seed treatment have a signficant positive effect
# 0.8544% of the variarion observed in yield is attributable to seed treatment

aggregate(df$Estimate,
          by=list(df$SeedTreatment),
          FUN=mean)
# Seed treatment C perfrom the best.
```


# 6. What is the best model for explaining the variation in the yield data,WITHOUT GENOS! 
# Which location should I choose to use with which company's product
# to get the maximum yield? Should I apply Seed Treatment or not?

## EE comment : The trick here was to test the full model with everything except the markers, with step()
If you have done that the best model is then Loc + variety
However, I will accept your answer based on the justification you provided.

```{r}
# what is the differnce
model6 <- lm(Estimate ~ Reg + Loc + Company + SeedTreatment, df)
anova(model6)

#check for multi-collinearity
model6.1 <- lm(Estimate ~ Company + Loc + Reg, df)
model6.2 <- lm(Estimate ~ Company + Reg + Loc, df)
model6.3 <- lm(Estimate ~ Company + Loc, df)
model6.4 <- lm(Estimate ~ Company + Reg, df)
anova(model6.1)
anova(model6.2)
anova(model6.1, model6.3, model6.4)
summary(model6.1)$adj.r.squared # 0.8250266
summary(model6.3)$adj.r.squared # 0.8250266
summary(model6.4)$adj.r.squared # 0.1910692

# it seems that there is muti-colinearity beteween Location and region, based on the variation explained, it is better to include Location in without region

as.data.frame(aggregate(df$Estimate,
                        by=list(df$Loc, df$Company), FUN=mean)) -> ag6



# linear model with company and location in is the best model
# at location Hampshire_2019, with the seed from Pioneer
# there is no need to apply seed treatment

```


# 7. Give your best prediction of maximum yield under these best case scenario conditions.
# Which location should I choose to use with which company's product
# to get the maximum yield? Should I apply Seed Treatment or not?
# Give your best prediction of maximum yield under these best case scenario conditions.
```{r}
new.df<- data.frame(
  Reg = "east", Company = "Pioneer", SeedTreatment = "C", Loc = "Hampshire_2019"
)
new.df1<- data.frame(
  Company = "Pioneer", Loc = "Hampshire_2019"
)

model6 <- lm(Estimate ~ Reg + Loc + Company + SeedTreatment, df)
model6.3 <- lm(Estimate ~ Company + Loc, df)

predict(model6, new.df)
predict(model6.3, new.df1)
# east	Hampshire_2019	DeRaedt_Seed	with seed treatment C
# 115.034
# 115.04 

# we should use company DeRaedt_Seed, at location Hampshire_2019 at east region.
# according to the predictions, we don't need to apply seed treatment.
# the best prediction of maximum yield is 115.04  
```
##EE comment : I don't exactly follow- how you got to this answer with the code you provided. You are looking up the performance for Pioneer varieties in Hampshire, but you are saying we should use DaRaedt_Seed?( -2 points)

## PART 2 ## Genetic Effects Evaluation
## Submit a visual and output for each question below

# The variables qmx1..qmx369  each represent a "genetic_marker" that was scored on 
# the entries that were included in the trials to identify if there are any genetic
# determinants that can be identified for high-yield.
# There are a total of 369 factor variables- with 2 states each {0,1} that are scored
# NOTE THAT YOU CAN NOT USE VARIETY AND GENOTYPE at the SAME TIME
# GENOTYPE marker data Defines VARIETY!!

# Using the example code for LASSO regression provided below - fit a LASSO model
# to the data and investigate these marker variables for potential significant positive effects.
# Give your best prediction of maximum yield for a hypothetical variety, under the best
# possible environmental conditions we can offer.

# 1. Does the model fit improve with marker data?
# 2. Generate a list of markers that seem to have significant positive effects on yield,
# and provide their coefficients.
```{r}
model_v <- lm(Estimate ~ Loc + Company + df$Variety, df)
anova(model6.3,model_v)

#Analysis of Variance Table

#Model 1: Estimate ~ Company + Loc
#Model 2: Estimate ~ Loc + Company + df$Variety
#  Res.Df    RSS  Df Sum of Sq      F    Pr(>F)    
#1    389 9555.6                                   
#2    280 5523.5 109    4032.1 1.8752 1.937e-05 ***
#---
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

## according to the redult of the ANOVA table, the incusion of variety (marker) would improve the model.

for (i in 8:376) {
  df[,i] <- as.factor(df[,i])
}


library(glmnet)
library(ISLR) 
df3 <- df[,-c(1,2,6,7)]
x = model.matrix(Estimate~., df3)[,-1]
y = df %>%
  dplyr::select(Estimate) %>%
  unlist() %>%
  as.numeric()

## Create a grid to investigate how changing lambda effects the outcomes
grid = 10^seq(10, -2, length = 100)
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge_mod))
plot(ridge_mod)

## evaluate the model coefficients at the 50th value for lambda 
ridge_mod$lambda[50] #[1] 11497.57
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm #[1] 0.04025229

## cross-validation to evaluate best lambda
## need to set seed for reproducible results
set.seed(13)
train = df3 %>%
  sample_frac(0.5)

test = df3 %>%
  setdiff(train)

x_train = model.matrix(Estimate~., train)[,-1]
x_test = model.matrix(Estimate~., test)[,-1]

y_train = train %>%
  dplyr::select(Estimate) %>%
  unlist() %>%
  as.numeric()

y_test = test %>%
  dplyr::select(Estimate) %>%
  unlist() %>%
  as.numeric()

## choose best lambda
set.seed(13)
# Fit ridge regression model on training data
cv.out = cv.glmnet(x_train, y_train, alpha = 0) 
# Select lamda that minimizes training MSE
bestlam = cv.out$lambda.min  
bestlam # [1] 67.89389

## fit ridge with best lambda
# Fit ridge regression model on full dataset
out = glmnet(x, y, alpha = 0) 
# Display coefficients using lambda chosen by CV
predict(out, type = "coefficients", s = bestlam)

### LASSO ###
## ridge regression is called l2 regularization while LASSO is called l1 regulatization
# with glmnet package - you can switch between the two with specifying alpha
# alpha = 0 --> ridge  alpha = 1 -->LASSO

lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   lambda = grid) # Fit lasso model on training data

plot(lasso_mod) 
set.seed(13)
cv.out = cv.glmnet(x_train, y_train, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test)

out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam) # Display coefficients using lambda chosen by CV
lasso_coef
lasso_coef[lasso_coef != 0] 
lasso_coef[lasso_coef > 0] [10:46]
```
EE comment : here the output can be saved as.matrix()- which will then have the names you can look up.
then you don't need to type it out like this

lcf<-as.data.frame(as.matrix(lasso_coef))
rownames(lcf)->lcf$nameList
colnames(lcf)<-c("coef","nameList")
lcf[which(lcf$coef != 0),]->non_zero
lcf[which(lcf$coef > 0),]->pos

(-2 points) coding style and accuracy

```{r}

# qxm11                              0.761275807
# qxm21                              0.535405159
# qxm31                              2.000046903
# qxm41                              1.456278689
# qxm51                              0.444783153
# qxm71                              0.401343884
# qxm81                              0.837898668
# qxm91                              0.386399832
# qxm111                             0.777121349
# qxm211                             0.220911937
# qxm231                             0.028597407
# qxm251                             0.223073431
# qxm291                             0.031565074
# qxm371                             0.022215812
# qxm761                             0.307809964
# qxm841                             0.295192132
# qxm991                             0.016651827                          .          
# qxm1311                            0.448033125
# qxm1391                            0.358516496
# qxm1521                            0.812642364
# qxm1641                            0.165084879
# qxm1901                            0.647043067
# qxm1911                            0.141561069
# qxm2271                            0.957403602
# qxm2281                            0.055337537
# qxm2731                            0.426362525
# qxm2771                            0.329033860
# qxm2831                            1.045761399
# qxm2911                            0.088331212
# qxm3101                            0.095444262
# qxm3111                            0.536438926
# qxm3321                            0.011844847
# qxm3331                            0.196866768
# qxm3351                            0.171541689
# qxm3571                            1.023888740
# qxm3611                            0.119371321
# qxm3661                            0.228437511


0.76127581+0.53540516+2.00004690+1.45627869+0.44478315+0.40134388+0.83789867+0.38639983+0.77712135+0.22091194+0.02859741+0.22307343+0.03156507+0.02221581+0.30780996+0.29519213+0.01665183+0.44803313+0.35851650+0.81264236+0.16508488+0.64704307+0.14156107+0.95740360+0.05533754+0.42636252+0.32903386+1.04576140+0.08833121+0.09544426+0.53643893+0.01184485+0.19686677+0.17154169+1.02388874+0.11937132+0.22843751 
# [1] 16.60552


```



## PART 3 ## Visualization
## Submit a visual and output for each question below

# 1. Yield Distributions by location 
# a. Density Plot

```{r}
library(ggplot2)
ggplot(df, aes(x=Estimate, fill=Loc)) + geom_density(alpha=0.5) +
  xlab("Location")
```

# b. box whisker plot by location
```{r}
ggplot(data=df, aes(x=Loc,y=Estimate)) + geom_boxplot() +
  xlab("Location")
```


# c. density plot by Company
```{r}
ggplot(df, aes(x=Estimate, fill=Company)) + geom_density(alpha=0.5) +
  xlab("Company")
```

# d. box whisker plot by Company ordered by median yield for company
```{r}
library(dplyr)
df2 <- df[,1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

df3 <- df2[order(df2$Com_median),]

ggplot(data=df3, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

# e. box whisker plot by Company ordered by median yield for company, for each location
```{r}
levels(df2$Loc)

# Belleville_2019
df2.1 <- df[df$Loc=="Belleville_2019",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.1, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Belleville_2019")

# Elkville_2019
df2.2 <- df[df$Loc=="Elkville_2019",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.2, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Elkville_2019")

# Hampshire_2019
df2.3 <- df[df$Loc=="Hampshire_2019",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.3, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Hampshire_2019")

# Neoga_2018
df2.4 <- df[df$Loc=="Neoga_2018",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.4, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Neoga_2018")

# Perry_2019
df2.5 <- df[df$Loc=="Neoga_2018",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.5, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Perry_2019")

# Urbana_2018
df2.6 <- df[df$Loc=="Urbana_2018",1:7] %>%
  group_by(Company) %>%
  mutate(Com_median=median(Estimate))

ggplot(data=df2.6, aes(x=fct_reorder(Company,Com_median),y=Estimate)) + geom_boxplot() +
  xlab("Company") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Yield Estimation for Urbana_2018")

```


## PART 4 # BONUS # LOGISTIC  LASSO  REGRESSION ###
# Create a new binary variable based on the "Company" variable,
# where Company(ies) whose Entries score as the top 5 performers across regions are coded as 1
# and the rest of the companies are coded as 0.
# Evaluate this variable versus the marker data [qmx1..qmx369] with ridge regression.
# Return the list of markers and their LASSO predicted coefficients from the model.

```{r}
Comp <- c("Pioneer", "AgriPro", "Moiner_See", "Dyna-Gro", "KWS_Cereals")
df4.1 <- df %>%
  mutate(Comp = as.factor(ifelse(df$Company %in% Comp, "1", "0")))

df4 <- df4.1[,-c(1,2,3,4,5,6,7)]
model_log <- glm(Comp ~ ., family = binomial(), data = df4)
```
#EE comment
Did you notice this model did not converge?
> model_log <- glm(Comp ~ ., family = binomial(), data = df4)
glm.fit: algorithm did not converge
You can not use coefficient estimates from a model that have not converged- it would be incorrect.
Also- you didn't really do the right LASSO model
it would be the same lasso model you had in part 2
here is your modified code from Part 2
start with the df4.1 you created above
Your response will be the newly created Comp variable

df4.2 <- df4.1[,8:377]

x = model.matrix(Comp~., df4.2)[,-1]
y = df4.2 %>%
  dplyr::select(Comp) %>%
  unlist() 
  
## Create a grid to investigate how changing lambda effects the outcomes
grid = 10^seq(10, -2, length = 100)

## cross-validation to evaluate best lambda
## need to set seed for reproducible results
set.seed(13)
train = df4.2 %>%
  sample_frac(0.5)

test = df4.2 %>%
  setdiff(train)

x_train = model.matrix(Comp~., train)[,-1]
x_test = model.matrix(Comp~., test)[,-1]

y_train = train %>%
  dplyr::select(Comp) %>%
  unlist() 
  
y_test = test %>%
  dplyr::select(Comp) %>%
  unlist() 
  
## choose best lambda- with the logistic regression model
set.seed(13)
# Fit ridge regression model on training data
cv.out = cv.glmnet(x_train, y_train,family="binomial", alpha = 1, lambda=grid) 
# Select lamda that minimizes training MSE
bestlam = cv.out$lambda.min  
bestlam 

lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   family="binomial",
                   lambda = grid) # Fit lasso model on training data

plot(lasso_mod) 
set.seed(13)

cv.out = cv.glmnet(x_train, y_train,family="binomial", alpha = 1) 
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min

lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test)

out = glmnet(x, y, 
              alpha = 1, 
              lambda = bestlam,
              family="binomial") # Fit lasso model on full dataset

lasso_coef = predict(out, type = "coefficients", s = bestlam)
lcf<-as.data.frame(as.matrix(lasso_coef))
rownames(lcf)->lcf$nameList
colnames(lcf)<-c("coef","nameList")
lcf[which(lcf$coef != 0),]->non_zero
lcf[which(lcf$coef > 0),]->pos


```{r}
library(glmnet)
library(ISLR) 

x = model.matrix(Comp~., df4)[,-1]
y = df4 %>%
  dplyr::select(Comp) %>%
  unlist() %>%
  as.numeric()

## Create a grid to investigate how changing lambda effects the outcomes
grid = 10^seq(10, -2, length = 100)
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge_mod))
plot(ridge_mod)

## evaluate the model coefficients at the 50th value for lambda 
ridge_mod$lambda[50] #[1] 11497.57
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm #[1] 0.04025229

## cross-validation to evaluate best lambda
## need to set seed for reproducible results
set.seed(13)
train = df4 %>%
  sample_frac(0.5)

test = df4 %>%
  setdiff(train)

x_train = model.matrix(Comp~., train)[,-1]
x_test = model.matrix(Comp~., test)[,-1]

y_train = train %>%
  dplyr::select(Comp) %>%
  unlist() %>%
  as.numeric()

y_test = test %>%
  dplyr::select(Comp) %>%
  unlist() %>%
  as.numeric()

## choose best lambda
set.seed(13)
# Fit ridge regression model on training data
cv.out = cv.glmnet(x_train, y_train, alpha = 0) 
# Select lamda that minimizes training MSE
bestlam = cv.out$lambda.min  
bestlam # [1] 67.89389

## fit ridge with best lambda
# Fit ridge regression model on full dataset
out = glmnet(x, y, alpha = 0) 
# Display coefficients using lambda chosen by CV
predict(out, type = "coefficients", s = bestlam)

### LASSO ###
## ridge regression is called l2 regularization while LASSO is called l1 regulatization
# with glmnet package - you can switch between the two with specifying alpha
# alpha = 0 --> ridge  alpha = 1 -->LASSO

lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   lambda = grid) # Fit lasso model on training data

plot(lasso_mod) 
set.seed(13)
cv.out = cv.glmnet(x_train, y_train, alpha = 1) # Fit lasso model on training data
plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlam = cv.out$lambda.min
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test)

out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam) # Display coefficients using lambda chosen by CV
lasso_coef
lasso_coef[lasso_coef != 0] 
```

